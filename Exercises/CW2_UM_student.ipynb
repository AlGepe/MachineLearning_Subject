{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresja logistyczna (W2), walidacja, krzywe ROC(W3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten notebook pomoże Ci zapoznać się z regresją logistyczną. \n",
    "\n",
    "Zbudujemy klasyfikator bazujący na regresji logistycznej. Jego zadaniem będzie określanie prawdopodobieństwa przyjęcia kandydata na studia na podstawie wyników z dwóch egzaminów maturalnych (każdy przeskalowany na zakres 0-100%): z matematyki i z biologii. \n",
    "\n",
    "Trzeba będzie uzupełnić kody funkcji:\n",
    "*     sigmoida\n",
    "*     funkcjaKosztu\n",
    "*     predykcja\n",
    "*     funkcjaKosztuReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zanim przejdziemy do właściwych zadań zaimportujmy potrzebne moduły:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "% matplotlib notebook\n",
    "#matplotlib.use('TkAgg')\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import scipy.optimize as so\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hipotetyczne dane znajdują się w pliku:\n",
    "\n",
    "https://brain.fuw.edu.pl/edu/images/d/d8/Reg_log_data1.txt.\n",
    "\n",
    "Proszę je pobrać i zapisać w bieżącym katalogu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Wczytanie danych\n",
    "\n",
    "Zawsze dobrze jest rozpocząć pracę od zapoznania się z danymi. W tym celu powinniśmy poznać strukturę danych: \n",
    "* Pierwsze dwie kolumny zawierają wyniki egzaminów,\n",
    "* trzecia kolumna zawiera etykietę (przynależność do grupy)\n",
    "\n",
    "Wczytajmy dane i zobaczmy pierwsze 10 linii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('reg_log_data1.txt',delimiter=',')\n",
    "print(data[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby łatwiej było się nimi posługiwać wydzielmy z nich dane wejściowe jako 'X' i wyjściowe jako 'Y':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data[:, [0, 1]]\n",
    "y = data[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zawsze dobrze jest pooglądać dane aby nabrać wyczucia do problemu\n",
    "Rysujemy dane: \n",
    "* żółty symbol oznacza przykłady z gdzie y = 1 zaś \n",
    "* niebieski te z y = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set3)\n",
    "plt.xlabel('wynik z matematyki')\n",
    "plt.ylabel('wynik z biologii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby wygodnie można było stosować oblicznia wektorowe do regresji logistycznej trzeba  zmodyfikować macierz X poprzez dodanie jej z lewej strony kolumny jedynek. Będą one mnożyły  parametr $\\theta_0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = len(y) # ilość przykładów w zbiorze uczącym\n",
    "XX = np.concatenate((np.ones((N,1)), X),axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zobaczmy jak teraz wygląda macierz XX:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(XX[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hipoteza\n",
    "Dla przypomnienia _hipoteza_ w regresji logistycznej ma postać: \n",
    "\n",
    "$\\qquad$ $h_\\theta(x) = \\frac{1}{1+\\exp(-\\theta^Tx)}$.\n",
    "\n",
    "W implementacji dobrze jest myśleć o tej funkcji tak:\n",
    "\n",
    "$\\qquad$ $h_\\theta(x) = \\frac{1}{1+f}$.\n",
    "\n",
    "gdzie: $f = \\exp(-\\theta^Tx)$\n",
    "\n",
    "* zaimplementuj  hipotezę dla regresji logistycznej, \n",
    "* ze względu na stabilność numeryczną obliczeń dobrze jest ograniczyć zakres zmienności $f$ np do zakresu [1e-8, 1e+8]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hipoteza(x, theta):\n",
    "    '''ta funkcja zwraca wartość hipotezy dla danego wejścia x i parametrów theta'''\n",
    "    f = np.exp(...)\n",
    "    if f < 1e-8:\n",
    "        f = 1e-8\n",
    "    if f>1e8:\n",
    "        f = 1e8 \n",
    "    h = 1/(1+f)   \n",
    "    return h\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przetestuj funkcję:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H0 = hipoteza(XX[0,:], np.zeros((3, 1));)\n",
    "print('wartość hipotezy dla zerowego przykładu i dla początkowej thety: '+ str(H0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powino wyjść:\n",
    "\n",
    "```wartość hipotezy dla zerowego przykładu i dla początkowej thety: [ 0.5]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcja log-wiarygodności: \n",
    "* parametry regresji znajdujemy przez maksymalizację [funkcji log-wiarygodności](https://brain.fuw.edu.pl/edu/index.php/Uczenie_maszynowe_i_sztuczne_sieci_neuronowe/Wykład_6#Funkcja_wiarygodno.C5.9Bci):\n",
    "\n",
    "$\\qquad$ $l(\\theta) = \\log L(\\theta) = \\sum_{j=1}^m y^{(j)} \\log h(x^{(j)}) + (1 - y^{(j)}) \\log (1 - h(x^{(j)}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def funkcjaLogWiarygodnosci(theta, X, y):\n",
    "    '''Ta funkcja oblicza wartość funkcji log-wiarygodności  dla regresji logistycznej\n",
    "    używając theta jako parametrów oraz X i y jako zbioru uczącego'''\n",
    "    l=0.0\n",
    "    # pętla po przykładach ze zbioru uczącego\n",
    "    for j in range(len(y)): \n",
    "        h = hipoteza(...)\n",
    "        l +=  ...   \n",
    "    return l   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tym ćwiczeniu zrobimy to za pomocą funkcji optymalizacyjnych z modułu [<tt>scipy.optimize</tt>]( http://docs.scipy.org/doc/scipy/reference/optimize.html#module-scipy.optimize). \n",
    "\n",
    "Wynikają z tego dwie konsekwencje:\n",
    "* Funkcje te są przystosowane do szukania minimów funkcji celu. Musimy więc podawać im jako argumenty funkcję minus log-wiarygodności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minusFunkcjaLogWiarygodnosci(theta, X, y):\n",
    "    return (-1.)*funkcjaLogWiarygodnosci(theta, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Niektóre algorytmy mogą działać szybciej jeśli zaimplementujemy jawnie postać pochodnej:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\begin{array}{lcl}\n",
    "\\frac{\\partial}{\\partial \\theta_i} l(\\theta)  =\\sum_{j=1}^m (y^{(j)}-h_\\theta(x^{(j)}))x_i^{(j)}\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pochodnaLogWiarygodnosci(theta, X, y):\n",
    "    '''ta funkcja oblicza wartość pochodnej funkcji log-wiarygodności\n",
    "    dla podaanych wartości theta, X i y'''\n",
    "    dl_dtheta = np.zeros(len(theta))\n",
    "    for i in range(len(theta)): # dla i-tej współrzędnej theta\n",
    "        for j in range(len(y)):  # dodajemy przyczynki od przykładu j-ego \n",
    "            h = ...\n",
    "            dl_dtheta[i] += ...\n",
    "    return dl_dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def  minusPochodnaLogWiarygodnosci(theta, X, y):\n",
    "    return (-1)*pochodnaLogWiarygodnosci(theta, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicjujemy parametry $\\theta$ na wartości 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xDim = XX.shape[1] # rozmiar wejścia rozszerzonego o jedynki\n",
    "theta0 = np.zeros((xDim, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zobaczmy jak wygląda początkowa $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obliczamy funkcje log-wiarygodności i jej pochodną dla danych początkowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logWiar = funkcjaLogWiarygodnosci(theta0, XX, y)\n",
    "pochLogWiar = pochodnaLogWiarygodnosci(theta0, XX, y)\n",
    "\n",
    "print( 'wartość log-wiarygodności dla początkowej thety: '+ str(logWiar))\n",
    "print( 'pochodna log-wiarygodnosci dla poczatkowej thety: '+ str(pochLogWiar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przy prawidłowej implementacji pownino wyjść:\n",
    "```\n",
    "wartość log-wiarygodności dla początkowej thety: [-69.31471806]\n",
    "pochodna log-wiarygodnosci dla poczatkowej thety: [   10.          1200.92165893  1126.28422055]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optymalizacja  \n",
    "\n",
    "* Funkcje optymalizujące zaczerpniemy z modułu scipy.optimize\n",
    "* Ponieważ funkcje te są zaimplementowane do mnimalizowania to zamiast maksymalizować funkcję low-wiarygodności będziemy minimalizować tą funkcje przemnożoną przez -1 czyli minusFunkcjaLogWiarygodnosci fprime=minusPochodnaLogWiarygodnosci,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_opt = so.fmin_bfgs(minusFunkcjaLogWiarygodnosci, theta0, \n",
    "                         fprime=minusPochodnaLogWiarygodnosci, \n",
    "                         args=(XX,y), disp= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wypiszmy dopasowane $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Wartość log wiarygodnosci  dla optymalnych parametrów: '+\n",
    "      str(funkcjaLogWiarygodnosci(theta_opt, XX, y)))\n",
    "print( 'theta: '+str(theta_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyniki\n",
    "Wyniki regresji logistycznej możemy odbierać na dwa sposoby:\n",
    "* obliczyć wartość hipotezy dla badanego wejścia i dopasowanych parametrów: miara ta ma interpretację prawdopodobieństwa przynależności wejścia do klasy 1,\n",
    "* dopisać funkcję wykonującą klasyfikację, tzn. porównanie wartości hipotezy z 1/2: \n",
    "  * dla wartości hipotezy > 1/2 klasyfikacja zwraca 1, \n",
    "  * w przeciwnym razie 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def klasyfikacja(testX, theta):\n",
    "    ''' Ta funkcja zwraca wynik klasyfikacji przykładu testX przy parametrach theta.\n",
    "    Po obliczeniu hipotezy, jeśli otrzymane prawdopodobieństwo jest większe niż 0.5 to \n",
    "    zwraca 1 w przeciwnym wypadku zwraca 0'''\n",
    "    h = ...\n",
    "    if h > ... :\n",
    "        klasa = ...\n",
    "    else:\n",
    "        klasa = ...\n",
    "        \n",
    "    return klasa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przewidywanie \n",
    "Po dopasowaniu parametrów nadszedł czas aby zrobić predykcję.\n",
    "Obliczmy jakie prawdopodobieństwo przyjęcia ma kandydat z wynikami\n",
    "* 20 z matematyki\n",
    "*  80 z biologii\n",
    "Do przewidywania wykorzystujemy funkcję hipoteza, bo zgodnie z naszą interpretacją daje ona prawdopodobieństwo przyjęcia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = hipoteza([1, 20, 80], theta_opt)\n",
    "print('dla kandydata z wymnikami 20 z matematyki i 80 z biologii prawdopodobieństwo przyjęcia wynosi: ' +str(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Używając funkcji <tt>klasyfikacja</tt> dostaniemy klasę:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klasa = klasyfikacja([1, 20, 80], theta_opt)\n",
    "print('kandydat zalicza się do klasy: ' +str(klasa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narysujmy uzyskany podział. Na tle punktów pokolorowanych zgodnie z przynalenością do klas dorysujemy prostą rozgraniczającą obszary \"1\" od \"0\".   Ma ona równanie \n",
    "\n",
    "$\\qquad$ $h_\\theta(x)=1/2$, \n",
    "\n",
    "tzn:\n",
    "\n",
    "$\\qquad$ $\\theta^T x = 0$\n",
    "\n",
    "czyli \n",
    "\n",
    "$\\theta_0 +\\theta_1 x_1 + \\theta_2 x_2 =0 $\n",
    "\n",
    "Przekształcając to do równania prostej we współrzędnych $(x_1,x_2)$ mamy:\n",
    "\n",
    "$- \\theta_2 x_2 = \\theta_0 +\\theta_1 x_1 $\n",
    "\n",
    "$ x_2 = - \\frac{1}{\\theta_2}( \\theta_0 +\\theta_1 x_1 )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set3)\n",
    "plt.xlabel('wynik z matematyki')\n",
    "plt.ylabel('wynik z biologii')\n",
    "\n",
    "x_plot = np.array([np.min(X[:,1]), np.max(X[:,1])])\n",
    "y_plot = -1./theta_opt[2]*(theta_opt[1]*x_plot + theta_opt[0])\n",
    "\n",
    "plt.plot(x_plot,y_plot,'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walidacja\n",
    "Teoria do tej części znajduje się tu:\n",
    "\n",
    "https://brain.fuw.edu.pl/edu/index.php/Uczenie_maszynowe_i_sztuczne_sieci_neuronowe/Wykład_Ocena_jakości_klasyfikacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zastosowanie w naszym przykładzie\n",
    "Dodamy teraz kross-walidację typu leave-one-out.\n",
    "Po kolei odłożymy po jednym przykładzie ze zbioru uczącego i na takim zredukownaym zbiorze nauczymy regresję, a następnie sprawdzimy która z poniższych możliwych sytuacji zachodzi:\n",
    "* TP:  stan faktyczny jest pozytywny (y=1) i klasyfikator się nie myli (wynik = 1)\n",
    "* TN:  stan faktyczny jest negatywny (y=0) i klasyfikator się nie myli (wynik = 0) \n",
    "* FP:   wynik fałszywie pozytywny (fałszywy alarm): stan faktyczny jest negatywny (y=0) ale klasyfikator się  myli (wynik = 1)\n",
    "* FN: przegapiony alarm: stan faktyczny jest pozytywny (y=1) i klasyfikator się myli (wynik = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przygotowujemy liczniki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TP = 0 \n",
    "TN = 0\n",
    "FP = 0 \n",
    "FN = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " W pętli odkładamy przykład `v` do testowania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in range(len(y)):\n",
    "    testX = XX[v]\n",
    "    testY = y[v]\n",
    "    # robimy zredukowany zbiór uczący przez usunięcie przykładu v\n",
    "    uczX = np.delete(XX,v,axis=0)\n",
    "    uczY = np.delete(y,v)\n",
    "    # uczymy regresję na zredukowanym zbiorze uczącym\n",
    "    theta_opt = so.fmin_bfgs(minusFunkcjaLogWiarygodnosci, theta0,\n",
    "                             fprime=minusPochodnaLogWiarygodnosci, \n",
    "                             args=(..., ...), disp= False)\n",
    "   # klasyfikujemy odłożony przykład : proszę uzupełnić funkcję klasyfikacja na początku pliku\n",
    "    wynik = klasyfikacja(... , ...)\n",
    "    # aktualizujemy liczniki; proszę uzupełnić kod:\n",
    "    if testY == 1 and wynik == 1:\n",
    "            ... += 1\n",
    "    if testY == 1 and wynik == 0:\n",
    "            ... +=1           \n",
    "    if testY == 0 and wynik == 1:\n",
    "            ... +=1\n",
    "    if testY == 0 and wynik == 0:\n",
    "            ... +=1\n",
    "print('TP: ', TP)\n",
    "print('FP: ', FP)\n",
    "print('TN: ', TN)\n",
    "print('FN: ', FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla naszego zbioru uczącego powinniśmy uzyskać:\n",
    "```\n",
    "TP:  55\n",
    "FP:  6\n",
    "TN:  34\n",
    "FN:  5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Krzywa ROC\n",
    "\n",
    "Aby wykreślić krzywą ROC należy przeprowadzić klasyfikację dla wielu możliwych wartości progu dla hipotezy, powyżej którego uznajemy przypadek za należący do klasy 1.\n",
    "\n",
    "\n",
    "Modyfikujemy funkcję klasyfikacja, tak aby wynik zależał też od progu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def klasyfikacjaProg(testX, theta,prog):\n",
    "    h = hipoteza(testX, theta)\n",
    "    if h > prog:\n",
    "        klasa = ...\n",
    "    else:\n",
    "        klasa = ...\n",
    "    return klasa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcję tą możemy wykorzystać do obliczenia liczebności poszczególnych przypadków klasyfikacji w zależności od progu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def liczROC(XX,y,progi):\n",
    "    '''funkcja oblicza FPR i TPR dla zadanych progów,\n",
    "       progi dla których mają być wyliczone wyniki podajemy w postaci wektora\n",
    "       '''\n",
    "    TP = np.zeros(len(progi))\n",
    "    TN = np.zeros(len(progi))\n",
    "    FP = np.zeros(len(progi))\n",
    "    FN = np.zeros(len(progi))\n",
    "    f = FloatProgress(min=0, max=len(y))\n",
    "    display(f)\n",
    "    for v in range(len(y)):\n",
    "        f.value+=1\n",
    "        \n",
    "        testX = XX[v]\n",
    "        testY = y[v]\n",
    "        tenX = np.delete(XX,v,axis=0)\n",
    "        tenY = np.delete(y,v)\n",
    "        theta_opt = so.fmin_bfgs(minusFunkcjaLogWiarygodnosci, theta0, \n",
    "                                 fprime=minusPochodnaLogWiarygodnosci, \n",
    "                                 args=(tenX,tenY), disp= False)\n",
    "        for ind, prog in enumerate(progi):\n",
    "            wynik = klasyfikacjaProg(testX, theta_opt,prog)\n",
    "           #==========================\n",
    "           #      tu wstaw odpowiedni kawałek kodu\n",
    "           #==========================\n",
    "            ...\n",
    "            \n",
    "    TPR = TP/(TP+FN)\n",
    "    FPR = FP/(FP+TN) \n",
    "    return (FPR,TPR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do wykreślenia krzywej ROC możesz użyć następującego kodu. Zaznaczamy w nim na wykresie wartości progów dla których osiągnięto konkretne wartości FPR i TPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progi = np.arange(0.0,1.1,0.1)\n",
    "FPR,TPR= liczROC(XX,y,progi)\n",
    "plt.figure()\n",
    "plt.plot(FPR,TPR,'o')\n",
    "plt.plot(FPR,TPR)\n",
    "for ind,pr in enumerate(progi):\n",
    "    plt.text(FPR[ind],TPR[ind],str(pr))\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.xlim((0,1))\n",
    "plt.ylim((0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policzmy jeszcze pole pod krzywą ROC, tzw. AUC (area under curve), za pomocą metody trapezów: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC = 0\n",
    "for ind in range(len(progi)-1):\n",
    "    a = ...\n",
    "    b = ...\n",
    "    h = ...\n",
    "    AUC += 0.5*(a+b)*h\n",
    "print('AUC: ',AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spodziewamy się wyniku:\n",
    "```\n",
    "AUC:  0.9375\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
