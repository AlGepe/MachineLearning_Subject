{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression (W2), validation, ROC curves (W3)\n",
    "\n",
    "This notebook will help you familiarize yourself with the logistics regression.\n",
    "\n",
    "We will build a classifier based on logistic regression. His task will be to determine the probability of admitting a candidate based on the results of two matura exams (each scaled to 0-100%): in mathematics and biology.\n",
    "\n",
    "You will need to complete the function codes:\n",
    "* sigmoida\n",
    "* cost function\n",
    "* prediction\n",
    "* functionCostageReg\n",
    "\n",
    "Before we get to the right tasks, we need to import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "% matplotlib notebook\n",
    "#matplotlib.use('TkAgg')\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import scipy.optimize as so\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothetical data can be found in the file:\n",
    "\n",
    "https://brain.fuw.edu.pl/edu/images/d/d8/Reg_log_data1.txt.\n",
    "\n",
    "Please download them and save in the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "It is always good to start work from getting acquainted with the data. To this end, we should know the data structure:\n",
    "* The first two columns contain the results of exams,\n",
    "* the third column contains a label (belonging to a group)\n",
    "\n",
    "Load the data and let's see the first 10 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('reg_log_data1.txt',delimiter=',')\n",
    "print(data[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier to use them, extracting input data from them as 'X' and output as 'Y':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data[:, [0, 1]]\n",
    "y = data[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always good to look at data to get a sense of the problem\n",
    "We draw data:\n",
    "* yellow symbol means examples from where y = 1 and\n",
    "* blue ones with y = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set3)\n",
    "plt.xlabel('wynik z matematyki')\n",
    "plt.ylabel('wynik z biologii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To comfortably apply vector calculations to logistic regression, you need to modify the matrix X by adding it to the left of the columns of ones. They will multiply the $ \\theta_0 $ parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = len(y) # ilość przykładów w zbiorze uczącym\n",
    "XX = np.concatenate((np.ones((N,1)), X),axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the XX matrix looks like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(XX[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "For the record, _hypothesis_ in logistic regression has the form:\n",
    "\n",
    "$\\qquad$ $h_\\theta(x) = \\frac{1}{1+\\exp(-\\theta^Tx)}$.\n",
    "\n",
    "In the implementation it is good to think about this function like this:\n",
    "\n",
    "$\\qquad$ $h_\\theta(x) = \\frac{1}{1+f}$.\n",
    "\n",
    "where: $f = \\exp(-\\theta^Tx)$\n",
    "\n",
    "* implement the hypothesis for logistic regression,\n",
    "* due to the numerical stability of calculations it is good to limit the range of volatility $ f $ np to the range [1e-8, 1e + 8]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hipoteza(x, theta):\n",
    "    '''ta funkcja zwraca wartość hipotezy dla danego wejścia x i parametrów theta'''\n",
    "    f = np.exp(...)\n",
    "    if f < 1e-8:\n",
    "        f = 1e-8\n",
    "    if f>1e8:\n",
    "        f = 1e8 \n",
    "    h = 1/(1+f)   \n",
    "    return h\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H0 = hipoteza(XX[0,:], np.zeros((3, 1));)\n",
    "print('wartość hipotezy dla zerowego przykładu i dla początkowej thety: '+ str(H0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should go:\n",
    "\n",
    "```wartość hipotezy dla zerowego przykładu i dla początkowej thety: [ 0.5]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-credibility function:\n",
    "* we find the regression parameters by maximizing [log-credibility function](https://brain.fuw.edu.pl/edu/index.php/Uczenie_maszynowe_i_sztuczne_sieci_neuronowe/Wykład_6#Funkcja_wiarygodno.C5.9Bci):\n",
    "\n",
    "$\\qquad$ $l(\\theta) = \\log L(\\theta) = \\sum_{j=1}^m y^{(j)} \\log h(x^{(j)}) + (1 - y^{(j)}) \\log (1 - h(x^{(j)}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def funkcjaLogWiarygodnosci(theta, X, y):\n",
    "    '''Ta funkcja oblicza wartość funkcji log-wiarygodności  dla regresji logistycznej\n",
    "    używając theta jako parametrów oraz X i y jako zbioru uczącego'''\n",
    "    l=0.0\n",
    "    # pętla po przykładach ze zbioru uczącego\n",
    "    for j in range(len(y)): \n",
    "        h = hipoteza(...)\n",
    "        l +=  ...   \n",
    "    return l   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will do it using the optimization functions from the module [scipy.optimize](http://docs.scipy.org/doc/scipy/reference/optimize.html#module-scipy.optimize) .\n",
    "\n",
    "There are two consequences:\n",
    "* These functions are adapted to look for the minima of the objective function. We must therefore give them as arguments a minus log-credibility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minusFunkcjaLogWiarygodnosci(theta, X, y):\n",
    "    return (-1.)*funkcjaLogWiarygodnosci(theta, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some algorithms may run faster if we implement the explicit derivative form:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\begin{array}{lcl}\n",
    "\\frac{\\partial}{\\partial \\theta_i} l(\\theta)  =\\sum_{j=1}^m (y^{(j)}-h_\\theta(x^{(j)}))x_i^{(j)}\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pochodnaLogWiarygodnosci(theta, X, y):\n",
    "    '''ta funkcja oblicza wartość pochodnej funkcji log-wiarygodności\n",
    "    dla podaanych wartości theta, X i y'''\n",
    "    dl_dtheta = np.zeros(len(theta))\n",
    "    for i in range(len(theta)): # dla i-tej współrzędnej theta\n",
    "        for j in range(len(y)):  # dodajemy przyczynki od przykładu j-ego \n",
    "            h = ...\n",
    "            dl_dtheta[i] += ...\n",
    "    return dl_dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def  minusPochodnaLogWiarygodnosci(theta, X, y):\n",
    "    return (-1)*pochodnaLogWiarygodnosci(theta, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize $ \\theta $ parameters at 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xDim = XX.shape[1] # rozmiar wejścia rozszerzonego o jedynki\n",
    "theta0 = np.zeros((xDim, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the initial looks like $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(theta0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate log-likelihood functions and its derivative for initial data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logWiar = funkcjaLogWiarygodnosci(theta0, XX, y)\n",
    "pochLogWiar = pochodnaLogWiarygodnosci(theta0, XX, y)\n",
    "\n",
    "print( 'wartość log-wiarygodności dla początkowej thety: '+ str(logWiar))\n",
    "print( 'pochodna log-wiarygodnosci dla poczatkowej thety: '+ str(pochLogWiar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the implementation is correct, the outputs will be:\n",
    "```\n",
    "wartość log-wiarygodności dla początkowej thety: [-69.31471806]\n",
    "pochodna log-wiarygodnosci dla poczatkowej thety: [   10.          1200.92165893  1126.28422055]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "* Optimization functions are taken from the scipy.optimize module\n",
    "* Because these functions are implemented to modernize, instead of maximizing the low-credibility function, we will minimize this function multiplied by -1 or minusLog FunctionTrialality fprime = minusLogalLogality,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_opt = so.fmin_bfgs(minusFunkcjaLogWiarygodnosci, theta0, \n",
    "                         fprime=minusPochodnaLogWiarygodnosci, \n",
    "                         args=(XX,y), disp= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write matched $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print( 'Wartość log wiarygodnosci  dla optymalnych parametrów: '+\n",
    "      str(funkcjaLogWiarygodnosci(theta_opt, XX, y)))\n",
    "print( 'theta: '+str(theta_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "We can receive logistic regression results in two ways:\n",
    "* calculate the hypothesis value for the tested input and the adjusted parameters: this measure has the interpretation of the probability of belonging to the class 1,\n",
    "* add the function performing the classification, i.e. comparison of the hypothesis value with 1/2:\n",
    "   * for the value of the hypothesis> 1/2, the classification returns 1,\n",
    "   * otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def klasyfikacja(testX, theta):\n",
    "    ''' Ta funkcja zwraca wynik klasyfikacji przykładu testX przy parametrach theta.\n",
    "    Po obliczeniu hipotezy, jeśli otrzymane prawdopodobieństwo jest większe niż 0.5 to \n",
    "    zwraca 1 w przeciwnym wypadku zwraca 0'''\n",
    "    h = ...\n",
    "    if h > ... :\n",
    "        klasa = ...\n",
    "    else:\n",
    "        klasa = ...\n",
    "        \n",
    "    return klasa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "After adjusting the parameters, it's time to make a prediction.\n",
    "Let's calculate the probability of acceptance of the candidate with the results\n",
    "* 20 in mathematics\n",
    "* 80 from biology\n",
    "To predict, we use the hypothesis function, because according to our interpretation it gives the probability of acceptance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob = hipoteza([1, 20, 80], theta_opt)\n",
    "print('dla kandydata z wymnikami 20 z matematyki i 80 z biologii prawdopodobieństwo przyjęcia wynosi: ' +str(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Używając funkcji <tt>klasyfikacja</tt> dostaniemy klasę:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "klasa = klasyfikacja([1, 20, 80], theta_opt)\n",
    "print('kandydat zalicza się do klasy: ' +str(klasa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw the obtained division. Against the background of points colored in accordance with belonging to classes, we draw a simple separating area \"1\" from \"0\". It has the equation\n",
    "\n",
    "$\\qquad $ $ h_ theta (x) = 1/2 $,\n",
    "\n",
    "ie:\n",
    "\n",
    "$ \\qquad $ $ \\theta ^ T x = 0 $\n",
    "\n",
    "or\n",
    "\n",
    "$ \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 = 0 $\n",
    "\n",
    "Converting this to the straight equation in the coordinates $ (x_1, x_2) $ we have:\n",
    "\n",
    "$ - \\theta_2 x_2 = \\theta_0 + \\theta_1 x_1 $\n",
    "\n",
    "$ x_2 = - \\frac {1} {\\theta_2} (\\theta_0 + \\theta_1 x_1) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set3)\n",
    "plt.xlabel('wynik z matematyki')\n",
    "plt.ylabel('wynik z biologii')\n",
    "\n",
    "x_plot = np.array([np.min(X[:,1]), np.max(X[:,1])])\n",
    "y_plot = -1./theta_opt[2]*(theta_opt[1]*x_plot + theta_opt[0])\n",
    "\n",
    "plt.plot(x_plot,y_plot,'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "The theory for this part can be found here:\n",
    "\n",
    "https://brain.fuw.edu.pl/edu/index.php/Uczenie_maszynowe_i_sztuczne_sieci_neuronowe/Wykład_Ocena_jakości_klasyfikacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application in our example\n",
    "We will now add a leave-one-out cross-validation.\n",
    "In turn, we will postpone one example from the training set, and on such a reduced set we will learn the regression, and then we will check which of the following possible situations occurs:\n",
    "* TP: the actual state is positive (y = 1) and the classifier is correct (result = 1)\n",
    "* TN: the actual state is negative (y = 0) and the classifier is correct (result = 0)\n",
    "* FP: false positives (false alarm): the actual state is negative (y = 0) but the classifier is wrong (result = 1)\n",
    "* FN: missed alarm: the actual state is positive (y = 1) and the classifier is wrong (result = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przygotowujemy liczniki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TP = 0 \n",
    "TN = 0\n",
    "FP = 0 \n",
    "FN = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " W pętli odkładamy przykład `v` do testowania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for v in range(len(y)):\n",
    "    testX = XX[v]\n",
    "    testY = y[v]\n",
    "    # robimy zredukowany zbiór uczący przez usunięcie przykładu v\n",
    "    uczX = np.delete(XX,v,axis=0)\n",
    "    uczY = np.delete(y,v)\n",
    "    # uczymy regresję na zredukowanym zbiorze uczącym\n",
    "    theta_opt = so.fmin_bfgs(minusFunkcjaLogWiarygodnosci, theta0,\n",
    "                             fprime=minusPochodnaLogWiarygodnosci, \n",
    "                             args=(..., ...), disp= False)\n",
    "   # klasyfikujemy odłożony przykład : proszę uzupełnić funkcję klasyfikacja na początku pliku\n",
    "    wynik = klasyfikacja(... , ...)\n",
    "    # aktualizujemy liczniki; proszę uzupełnić kod:\n",
    "    if testY == 1 and wynik == 1:\n",
    "            ... += 1\n",
    "    if testY == 1 and wynik == 0:\n",
    "            ... +=1           \n",
    "    if testY == 0 and wynik == 1:\n",
    "            ... +=1\n",
    "    if testY == 0 and wynik == 0:\n",
    "            ... +=1\n",
    "print('TP: ', TP)\n",
    "print('FP: ', FP)\n",
    "print('TN: ', TN)\n",
    "print('FN: ', FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our training set, we should get:\n",
    "```\n",
    "TP:  55\n",
    "FP:  6\n",
    "TN:  34\n",
    "FN:  5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve\n",
    "\n",
    "To plot a ROC curve, a classification should be made for a number of possible threshold values for the hypothesis above which we consider the case to be in Class 1.\n",
    "\n",
    "\n",
    "We modify the classification function so that the result also depends on the threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def klasyfikacjaProg(testX, theta,prog):\n",
    "    h = hipoteza(testX, theta)\n",
    "    if h > prog:\n",
    "        klasa = ...\n",
    "    else:\n",
    "        klasa = ...\n",
    "    return klasa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this function to calculate the number of individual classification cases depending on the threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def liczROC(XX,y,progi):\n",
    "    '''funkcja oblicza FPR i TPR dla zadanych progów,\n",
    "       progi dla których mają być wyliczone wyniki podajemy w postaci wektora\n",
    "       '''\n",
    "    TP = np.zeros(len(progi))\n",
    "    TN = np.zeros(len(progi))\n",
    "    FP = np.zeros(len(progi))\n",
    "    FN = np.zeros(len(progi))\n",
    "    f = FloatProgress(min=0, max=len(y))\n",
    "    display(f)\n",
    "    for v in range(len(y)):\n",
    "        f.value+=1\n",
    "        \n",
    "        testX = XX[v]\n",
    "        testY = y[v]\n",
    "        tenX = np.delete(XX,v,axis=0)\n",
    "        tenY = np.delete(y,v)\n",
    "        theta_opt = so.fmin_bfgs(minusFunkcjaLogWiarygodnosci, theta0, \n",
    "                                 fprime=minusPochodnaLogWiarygodnosci, \n",
    "                                 args=(tenX,tenY), disp= False)\n",
    "        for ind, prog in enumerate(progi):\n",
    "            wynik = klasyfikacjaProg(testX, theta_opt,prog)\n",
    "           #==========================\n",
    "           #      tu wstaw odpowiedni kawałek kodu\n",
    "           #==========================\n",
    "            ...\n",
    "            \n",
    "    TPR = TP/(TP+FN)\n",
    "    FPR = FP/(FP+TN) \n",
    "    return (FPR,TPR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code to plot the ROC curve. In the graph, we mark the threshold values for which specific FPR and TPR values have been achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "progi = np.arange(0.0,1.1,0.1)\n",
    "FPR,TPR= liczROC(XX,y,progi)\n",
    "plt.figure()\n",
    "plt.plot(FPR,TPR,'o')\n",
    "plt.plot(FPR,TPR)\n",
    "for ind,pr in enumerate(progi):\n",
    "    plt.text(FPR[ind],TPR[ind],str(pr))\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.xlim((0,1))\n",
    "plt.ylim((0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the field under the ROC curve, so-called AUC (area under curve), using the trapezoidal method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AUC = 0\n",
    "for ind in range(len(progi)-1):\n",
    "    a = ...\n",
    "    b = ...\n",
    "    h = ...\n",
    "    AUC += 0.5*(a+b)*h\n",
    "print('AUC: ',AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the result:\n",
    "```\n",
    "AUC:  0.9375\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
